# word_vectors

Training and evaluation pipelines for *Glove* and *Word2vec*.

## Требования

Понадобятся *Python* версии 3 и пакет `gensim`. Для подготовки трейнсета нужны
пакеты `corpuscula`, `nltk` и `toxine`. После установки `nltk` запустите:
```sh
$ python
>>> import nltk
>>> nltk.download('punkt')
>>> exit()
```

## Использование

```sh
python parse_wiki.py
```
Подготовка трейнсета для векторизации на базе русской Википедии. На выходе -
файл с токенизированными предложениями (токены разделены пробелом). Если
работаем с полем *FORM* или *LEMMA* и в поле *MISC* встретился признак
*Entity*, то заменяем токен на `'<EntityX>'`, где `X` - значение признака.
Токены с недопустимыми символами также замеяются на спец. токены вида `<Y>`,
где `Y` - тип спец. токена.

Работать будет долго. Рекомендуется вначале скачать и токенизировать Википедию
средствами `corpuscula` и `toxine`, а потом прописать в скрипт в переменную
`corpus` имя получившегося *CoNLL-U*-файла. Тогда этот промежуточный файл
можно будет использовать в дальнейшем для других задач.

Внутри каталога `glove`:
```sh
sh run_glove_train.sh
```
Тренировка векторов *Glove*. Длительность `1000` эпох, сохранение через каждые
`20` (см. переменные `EPOCHS` и `CHECKPOINT_EVERY` в скрипте).

Внутри каталога `w2v`:
```sh
sh run_w2v_train.sh
```
Тренировка векторов *Word2vec*. Длительность `1000` эпох, сохранение через
каждые `20` (см. переменные `EPOCHS`, `MAX_EPOCHS` и `CHECKPOINT_EVERY` в
скрипте).

По умолчанию используется режим *CBOW*. Чтобы включить *Skip-gram*, установите
переменную `SG` в `1`.

В *CBOW* по умолчанию контекстные вектора усредняются. Если нужно
суммирование, установите переменную `CBOW_SUM` в `1`.

---

Результаты обучения можно в первом приближении проверить при помощи
соответствующих пайплайнов, приведённых в ноутбуке `wv_evaluate.ipynb`.

## License

***word_vectors*** is released under the Creative Commons License. See the
[LICENSE](https://github.com/fostroll/word_vectors/blob/master/LICENSE) file
for more details.
